---
title: "ML_PS3"
author: "Xuefeng Xu"
date: "October 23, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r results='hide'}
library(tidyverse)
library(glmnet)
library(knitr)
library(Lahman)
library(dplyr)
library(class)
library(DMwR)
library(ggplot2)
library(rmarkdown)
```

```{r}
load('market_level.R') # market-level data is datam
load('market_airline_level.R') # market airline level data
```

# 0

Set the seed to 0 and randomly allocate 1,000 rows of the market-level data to a test set, to be used only in (7). Use the rest to do the following.

```{r}
set.seed(0)
test_rows<- sample(nrow(datam),1000) #randomly choose 1000 rows in mkt-level data
test <- datam[test_rows,] # this test set contains 1000 rows chosen above
train <- datam[-test_rows,] # this df set contains the rest rows

```

# 1

Estimate a linear probability model, predicting whether American Airlines enters a market as a function of the number of competitors. Note: American Airlines’ ticket carrier id is “AA”.


```{r}
AA <- #find out which market does American Airline enter
  datama %>%
  filter(ticket_carrier=="AA") %>%
  select(-price:-market_income)

train_AA <- left_join(train,AA, by=c("origin_airport_id"="origin_airport_id",
                             "dest_airport_id"="dest_airport_id"))

train_AA$AA <- 1*(train_AA$ticket_carrier=="AA") #create an indicator which equals to 1 if AA is in this mkt
train_AA <- select(train_AA,-ticket_carrier)
train_AA[is.na(train_AA)] <- 0 #fill N/A with 0
colnames(train_AA)
test_AA<-left_join(test,AA, by=c("origin_airport_id"="origin_airport_id",
                             "dest_airport_id"="dest_airport_id"))
test_AA$AA = 1*(test_AA$ticket_carrier=="AA")
test_AA[is.na(test_AA)] <- 0
colnames(test_AA)
```

```{r}
train_AA <- train_AA %>% mutate(comp = num_carriers - AA)
test_AA <- test_AA %>% mutate(comp = num_carriers - AA)

linear_m <- lm(AA~comp, train_AA)
summary(linear_m)
```

# 2

Repeat (1) using a logit model instead of a linear probability model.

```{r}
logit_m <- glm(AA ~ comp,family=binomial(link='logit'),data=train_AA)
summary(logit_m)
```

# 3

Repeat (1) using a probit model instead of a linear probability model.

```{r}
probit_m <- glm(AA ~ comp,family=binomial(link='probit'),data=train_AA)
summary(logit_m)
```

# 4

Compute non-parametric estimates of the conditional probabilities of entering. (ie compute the conditional probability of entering conditional on each number of competitors directly from the data).

```{r}
uni <- sort(unique(train_AA$comp))
for(i in uni){
 temp = train_AA %>% filter(comp == i)
 cat("The conditional probability of entering condition on ", i, " is: ")
 cat(sum(temp$AA)/nrow(temp), "\n")
}

```

# 5 

Plot the ﬁtted values of each regression in one graph (i.e. estimated probabilities on the y-axis and the number of competitors on the x-axis). In words, explain the coeﬃcients of the ﬁrst three models. How do the estimated relationships compare? Should we interpret these relationships causally? Are the estimates for the probit and logit similar? Should we have expected this ex ante?

```{r}
predicted.data <- as.data.frame(predict(logit_m, newdata = train_AA, 
                                        type="link", se=TRUE))
new.data <- cbind(train_AA, predicted.data)
new.data$fit <- logit_m$family$linkinv(new.data$fit)
predicted.data <- as.data.frame(predict(probit_m, newdata = train_AA, 
                                        type="link", se=TRUE))
new.data1 <- cbind(train_AA, predicted.data)
new.data1$fit <- probit_m$family$linkinv(new.data$fit)
predicted.data <- as.data.frame(predict(linear_m, newdata = train_AA, 
                                        type="response", se=TRUE))
p <- ggplot(train_AA, aes(comp, AA))
predicted.data <- as.data.frame(predict(linear_m, newdata = train_AA, 
                                        type="response", se=TRUE))
new.data2 <- cbind(train_AA, predicted.data)
p + geom_point() + geom_line(data=new.data, aes(y=fit)) + geom_line(data=new.data1, aes(y=fit)) + geom_line(data = new.data2, aes(y = fit, x= comp)) + ylim(0,1)
```

# 6 

Obviously other covariates matter in predicting whether or not American will enter a particular route. In addition to the number of competitors, add the average market distance, market size, hub route indicator, vacation route indicator, slot controlled indicator, and market income to the set of predictors. Fit to the data L1 regularized logistic regression (ie Lasso for logit, pg 125-126 ESL) where the full model includes all squared terms and second-order cross terms. Using a 10-fold cross validation procedure, ﬁnd the optimal value of lambda.

# 7 

Calculate the sum of squared prediction errors on the test set for each of your 5 models and put them in a table. Explain your results.

